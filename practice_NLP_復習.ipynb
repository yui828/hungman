{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practice_NLP 復習.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9BRK6o3SUSBW",
        "FffRj31dUSBZ",
        "kaNBgLHVUSBa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yui828/hungman/blob/master/practice_NLP_%E5%BE%A9%E7%BF%92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3my0cErlPIN"
      },
      "source": [
        "# Pythonで実践する自然言語処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KaS7AdTUSBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19387309-ed3b-43f8-fc28-66972e4f4012"
      },
      "source": [
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aptitude-common libcgi-fast-perl libcgi-pm-perl libclass-accessor-perl\n",
            "  libcwidget3v5 libencode-locale-perl libfcgi-perl libhtml-parser-perl\n",
            "  libhtml-tagset-perl libhttp-date-perl libhttp-message-perl libio-html-perl\n",
            "  libio-string-perl liblwp-mediatypes-perl libparse-debianchangelog-perl\n",
            "  libsigc++-2.0-0v5 libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "Suggested packages:\n",
            "  aptitude-doc-en | aptitude-doc apt-xapian-index debtags tasksel\n",
            "  libcwidget-dev libdata-dump-perl libhtml-template-perl libxml-simple-perl\n",
            "  libwww-perl xapian-tools\n",
            "The following NEW packages will be installed:\n",
            "  aptitude aptitude-common libcgi-fast-perl libcgi-pm-perl\n",
            "  libclass-accessor-perl libcwidget3v5 libencode-locale-perl libfcgi-perl\n",
            "  libhtml-parser-perl libhtml-tagset-perl libhttp-date-perl\n",
            "  libhttp-message-perl libio-html-perl libio-string-perl\n",
            "  liblwp-mediatypes-perl libparse-debianchangelog-perl libsigc++-2.0-0v5\n",
            "  libsub-name-perl libtimedate-perl liburi-perl libxapian30\n",
            "0 upgraded, 21 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 3,877 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude-common all 0.8.10-6ubuntu1 [1,014 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigc++-2.0-0v5 amd64 2.10.0-2 [10.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcwidget3v5 amd64 0.5.17-7 [286 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxapian30 amd64 1.4.5-1ubuntu0.1 [631 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 aptitude amd64 0.8.10-6ubuntu1 [1,269 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-tagset-perl all 3.20-3 [12.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 liburi-perl all 1.73-1 [77.2 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhtml-parser-perl amd64 3.72-3build1 [85.9 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-pm-perl all 4.38-1 [185 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfcgi-perl amd64 0.78-2build1 [32.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libcgi-fast-perl all 1:2.13-1 [9,940 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsub-name-perl amd64 0.21-1build1 [11.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 libclass-accessor-perl all 0.51-1 [21.2 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libencode-locale-perl all 1.05-1 [12.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-date-perl all 6.02-1 [10.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-html-perl all 1.001-1 [14.9 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 liblwp-mediatypes-perl all 6.02-1 [21.7 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 libhttp-message-perl all 6.14-1 [72.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 libio-string-perl all 1.08-3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 libparse-debianchangelog-perl all 1.2.0-12 [49.5 kB]\n",
            "Fetched 3,877 kB in 2s (2,327 kB/s)\n",
            "Selecting previously unselected package aptitude-common.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../00-aptitude-common_0.8.10-6ubuntu1_all.deb ...\n",
            "Unpacking aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libsigc++-2.0-0v5:amd64.\n",
            "Preparing to unpack .../01-libsigc++-2.0-0v5_2.10.0-2_amd64.deb ...\n",
            "Unpacking libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Selecting previously unselected package libcwidget3v5:amd64.\n",
            "Preparing to unpack .../02-libcwidget3v5_0.5.17-7_amd64.deb ...\n",
            "Unpacking libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Selecting previously unselected package libxapian30:amd64.\n",
            "Preparing to unpack .../03-libxapian30_1.4.5-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Selecting previously unselected package aptitude.\n",
            "Preparing to unpack .../04-aptitude_0.8.10-6ubuntu1_amd64.deb ...\n",
            "Unpacking aptitude (0.8.10-6ubuntu1) ...\n",
            "Selecting previously unselected package libhtml-tagset-perl.\n",
            "Preparing to unpack .../05-libhtml-tagset-perl_3.20-3_all.deb ...\n",
            "Unpacking libhtml-tagset-perl (3.20-3) ...\n",
            "Selecting previously unselected package liburi-perl.\n",
            "Preparing to unpack .../06-liburi-perl_1.73-1_all.deb ...\n",
            "Unpacking liburi-perl (1.73-1) ...\n",
            "Selecting previously unselected package libhtml-parser-perl.\n",
            "Preparing to unpack .../07-libhtml-parser-perl_3.72-3build1_amd64.deb ...\n",
            "Unpacking libhtml-parser-perl (3.72-3build1) ...\n",
            "Selecting previously unselected package libcgi-pm-perl.\n",
            "Preparing to unpack .../08-libcgi-pm-perl_4.38-1_all.deb ...\n",
            "Unpacking libcgi-pm-perl (4.38-1) ...\n",
            "Selecting previously unselected package libfcgi-perl.\n",
            "Preparing to unpack .../09-libfcgi-perl_0.78-2build1_amd64.deb ...\n",
            "Unpacking libfcgi-perl (0.78-2build1) ...\n",
            "Selecting previously unselected package libcgi-fast-perl.\n",
            "Preparing to unpack .../10-libcgi-fast-perl_1%3a2.13-1_all.deb ...\n",
            "Unpacking libcgi-fast-perl (1:2.13-1) ...\n",
            "Selecting previously unselected package libsub-name-perl.\n",
            "Preparing to unpack .../11-libsub-name-perl_0.21-1build1_amd64.deb ...\n",
            "Unpacking libsub-name-perl (0.21-1build1) ...\n",
            "Selecting previously unselected package libclass-accessor-perl.\n",
            "Preparing to unpack .../12-libclass-accessor-perl_0.51-1_all.deb ...\n",
            "Unpacking libclass-accessor-perl (0.51-1) ...\n",
            "Selecting previously unselected package libencode-locale-perl.\n",
            "Preparing to unpack .../13-libencode-locale-perl_1.05-1_all.deb ...\n",
            "Unpacking libencode-locale-perl (1.05-1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package libhttp-date-perl.\n",
            "Preparing to unpack .../15-libhttp-date-perl_6.02-1_all.deb ...\n",
            "Unpacking libhttp-date-perl (6.02-1) ...\n",
            "Selecting previously unselected package libio-html-perl.\n",
            "Preparing to unpack .../16-libio-html-perl_1.001-1_all.deb ...\n",
            "Unpacking libio-html-perl (1.001-1) ...\n",
            "Selecting previously unselected package liblwp-mediatypes-perl.\n",
            "Preparing to unpack .../17-liblwp-mediatypes-perl_6.02-1_all.deb ...\n",
            "Unpacking liblwp-mediatypes-perl (6.02-1) ...\n",
            "Selecting previously unselected package libhttp-message-perl.\n",
            "Preparing to unpack .../18-libhttp-message-perl_6.14-1_all.deb ...\n",
            "Unpacking libhttp-message-perl (6.14-1) ...\n",
            "Selecting previously unselected package libio-string-perl.\n",
            "Preparing to unpack .../19-libio-string-perl_1.08-3_all.deb ...\n",
            "Unpacking libio-string-perl (1.08-3) ...\n",
            "Selecting previously unselected package libparse-debianchangelog-perl.\n",
            "Preparing to unpack .../20-libparse-debianchangelog-perl_1.2.0-12_all.deb ...\n",
            "Unpacking libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhtml-tagset-perl (3.20-3) ...\n",
            "Setting up libxapian30:amd64 (1.4.5-1ubuntu0.1) ...\n",
            "Setting up libencode-locale-perl (1.05-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libio-html-perl (1.001-1) ...\n",
            "Setting up aptitude-common (0.8.10-6ubuntu1) ...\n",
            "Setting up liblwp-mediatypes-perl (6.02-1) ...\n",
            "Setting up liburi-perl (1.73-1) ...\n",
            "Setting up libhtml-parser-perl (3.72-3build1) ...\n",
            "Setting up libcgi-pm-perl (4.38-1) ...\n",
            "Setting up libio-string-perl (1.08-3) ...\n",
            "Setting up libsub-name-perl (0.21-1build1) ...\n",
            "Setting up libfcgi-perl (0.78-2build1) ...\n",
            "Setting up libsigc++-2.0-0v5:amd64 (2.10.0-2) ...\n",
            "Setting up libclass-accessor-perl (0.51-1) ...\n",
            "Setting up libhttp-date-perl (6.02-1) ...\n",
            "Setting up libcgi-fast-perl (1:2.13-1) ...\n",
            "Setting up libparse-debianchangelog-perl (1.2.0-12) ...\n",
            "Setting up libhttp-message-perl (6.14-1) ...\n",
            "Setting up libcwidget3v5:amd64 (0.5.17-7) ...\n",
            "Setting up aptitude (0.8.10-6ubuntu1) ...\n",
            "update-alternatives: using /usr/bin/aptitude-curses to provide /usr/bin/aptitude (aptitude) in auto mode\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.8)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.13)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc{a} libmagic1{a} libmecab-dev libmecab2{a} mecab mecab-ipadic{a} mecab-ipadic-utf8 mecab-jumandic{a} mecab-jumandic-utf8{a} mecab-utils{a} \n",
            "0 packages upgraded, 11 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 29.3 MB of archives. After unpacking 282 MB will be used.\n",
            "Get: 1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get: 2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get: 3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get: 4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab2 amd64 0.996-5 [257 kB]\n",
            "Get: 5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libmecab-dev amd64 0.996-5 [308 kB]\n",
            "Get: 6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-utils amd64 0.996-5 [4,856 B]\n",
            "Get: 7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic-utf8 all 7.0-20130310-4 [16.2 MB]\n",
            "Get: 8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-jumandic all 7.0-20130310-4 [2,212 B]\n",
            "Get: 9 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic all 2.7.0-20070801+main-1 [12.1 MB]\n",
            "Get: 10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab amd64 0.996-5 [132 kB]\n",
            "Get: 11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 mecab-ipadic-utf8 all 2.7.0-20070801+main-1 [3,522 B]\n",
            "Fetched 29.3 MB in 3s (11.3 MB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 161296 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmecab2:amd64.\n",
            "Preparing to unpack .../03-libmecab2_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab2:amd64 (0.996-5) ...\n",
            "Selecting previously unselected package libmecab-dev.\n",
            "Preparing to unpack .../04-libmecab-dev_0.996-5_amd64.deb ...\n",
            "Unpacking libmecab-dev (0.996-5) ...\n",
            "Selecting previously unselected package mecab-utils.\n",
            "Preparing to unpack .../05-mecab-utils_0.996-5_amd64.deb ...\n",
            "Unpacking mecab-utils (0.996-5) ...\n",
            "Selecting previously unselected package mecab-jumandic-utf8.\n",
            "Preparing to unpack .../06-mecab-jumandic-utf8_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-jumandic.\n",
            "Preparing to unpack .../07-mecab-jumandic_7.0-20130310-4_all.deb ...\n",
            "Unpacking mecab-jumandic (7.0-20130310-4) ...\n",
            "Selecting previously unselected package mecab-ipadic.\n",
            "Preparing to unpack .../08-mecab-ipadic_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Selecting previously unselected package mecab.\n",
            "Preparing to unpack .../09-mecab_0.996-5_amd64.deb ...\n",
            "Unpacking mecab (0.996-5) ...\n",
            "Selecting previously unselected package mecab-ipadic-utf8.\n",
            "Preparing to unpack .../10-mecab-ipadic-utf8_2.7.0-20070801+main-1_all.deb ...\n",
            "Unpacking mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Setting up libmecab2:amd64 (0.996-5) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-utils (0.996-5) ...\n",
            "Setting up mecab-ipadic (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up libmecab-dev (0.996-5) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up mecab-jumandic-utf8 (7.0-20130310-4) ...\n",
            "Compiling Juman dictionary for Mecab.\n",
            "reading /usr/share/mecab/dic/juman/unk.def ... 37\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/Demonstrative.csv ... 97\n",
            "reading /usr/share/mecab/dic/juman/Wikipedia.csv ... 167709\n",
            "reading /usr/share/mecab/dic/juman/Noun.koyuu.csv ... 7964\n",
            "reading /usr/share/mecab/dic/juman/Noun.keishiki.csv ... 8\n",
            "reading /usr/share/mecab/dic/juman/Prefix.csv ... 90\n",
            "reading /usr/share/mecab/dic/juman/ContentW.csv ... 551145\n",
            "reading /usr/share/mecab/dic/juman/Noun.suusi.csv ... 49\n",
            "reading /usr/share/mecab/dic/juman/Postp.csv ... 108\n",
            "reading /usr/share/mecab/dic/juman/Suffix.csv ... 2128\n",
            "reading /usr/share/mecab/dic/juman/Noun.hukusi.csv ... 81\n",
            "reading /usr/share/mecab/dic/juman/Auto.csv ... 18931\n",
            "reading /usr/share/mecab/dic/juman/Emoticon.csv ... 972\n",
            "reading /usr/share/mecab/dic/juman/Assert.csv ... 34\n",
            "reading /usr/share/mecab/dic/juman/Rengo.csv ... 1118\n",
            "reading /usr/share/mecab/dic/juman/Special.csv ... 158\n",
            "reading /usr/share/mecab/dic/juman/AuxV.csv ... 593\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/juman/matrix.def ... 1876x1876\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-ipadic-utf8 (2.7.0-20070801+main-1) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "update-alternatives: using /var/lib/mecab/dic/ipadic-utf8 to provide /var/lib/mecab/dic/debian (mecab-dictionary) in auto mode\n",
            "Setting up mecab (0.996-5) ...\n",
            "Compiling IPA dictionary for Mecab.  This takes long time...\n",
            "reading /usr/share/mecab/dic/ipadic/unk.def ... 40\n",
            "emitting double-array: 100% |###########################################| \n",
            "/usr/share/mecab/dic/ipadic/model.def is not found. skipped.\n",
            "reading /usr/share/mecab/dic/ipadic/Others.csv ... 2\n",
            "reading /usr/share/mecab/dic/ipadic/Conjunction.csv ... 171\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.name.csv ... 34202\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.csv ... 60477\n",
            "reading /usr/share/mecab/dic/ipadic/Interjection.csv ... 252\n",
            "reading /usr/share/mecab/dic/ipadic/Auxil.csv ... 199\n",
            "reading /usr/share/mecab/dic/ipadic/Prefix.csv ... 221\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.proper.csv ... 27327\n",
            "reading /usr/share/mecab/dic/ipadic/Postp.csv ... 146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.demonst.csv ... 120\n",
            "reading /usr/share/mecab/dic/ipadic/Suffix.csv ... 1393\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.place.csv ... 72999\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adverbal.csv ... 795\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.verbal.csv ... 12146\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.org.csv ... 16668\n",
            "reading /usr/share/mecab/dic/ipadic/Adnominal.csv ... 135\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.nai.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Symbol.csv ... 208\n",
            "reading /usr/share/mecab/dic/ipadic/Postp-col.csv ... 91\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.adjv.csv ... 3328\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.number.csv ... 42\n",
            "reading /usr/share/mecab/dic/ipadic/Adverb.csv ... 3032\n",
            "reading /usr/share/mecab/dic/ipadic/Adj.csv ... 27210\n",
            "reading /usr/share/mecab/dic/ipadic/Filler.csv ... 19\n",
            "reading /usr/share/mecab/dic/ipadic/Verb.csv ... 130750\n",
            "reading /usr/share/mecab/dic/ipadic/Noun.others.csv ... 151\n",
            "emitting double-array: 100% |###########################################| \n",
            "reading /usr/share/mecab/dic/ipadic/matrix.def ... 1316x1316\n",
            "emitting matrix      : 100% |###########################################| \n",
            "\n",
            "done!\n",
            "Setting up mecab-jumandic (7.0-20130310-4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "                            \n",
            "Collecting mecab-python3==0.7\n",
            "  Downloading mecab-python3-0.7.tar.gz (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 559 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mecab-python3\n",
            "  Building wheel for mecab-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mecab-python3: filename=mecab_python3-0.7-cp37-cp37m-linux_x86_64.whl size=156607 sha256=a8f320a149fcdecaf3b6cb9fcf6f201c610e991c5ca24baed77d94e8c9b9eefd\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/46/95/3748ec2c4936cb69ee4d248a85e862064ea1e84819344c5292\n",
            "Successfully built mecab-python3\n",
            "Installing collected packages: mecab-python3\n",
            "Successfully installed mecab-python3-0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-l7eTmUSBI"
      },
      "source": [
        "import MeCab\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import unicodedata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6CRm0V5lPIV"
      },
      "source": [
        "## データの読み込み\n",
        "---\n",
        "今回の利用するデータセットは、livedoor ニュースコーパスの一部です。\n",
        "\n",
        "記事ファイル内容の提供元：  \n",
        "ITライフハック(label:0) http://news.livedoor.com/category/vender/223/  \n",
        "MOVIE ENTER(label:1) http://news.livedoor.com/category/vender/movie_enter/  \n",
        "Peachy(label:2) http://news.livedoor.com/category/vender/ldgirls/  \n",
        "Sports Watch(label:3) http://news.livedoor.com/category/vender/208/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qnIoNqbUSBJ",
        "outputId": "8ccabd77-04ec-446e-84ad-a0f6b0cd9f81"
      },
      "source": [
        "# データのダウンロード\n",
        "!wget https://basic-unstructured-data.s3-ap-northeast-1.amazonaws.com/unstructured-data-day2/data.zip\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-08 11:13:46--  https://basic-unstructured-data.s3-ap-northeast-1.amazonaws.com/unstructured-data-day2/data.zip\n",
            "Resolving basic-unstructured-data.s3-ap-northeast-1.amazonaws.com (basic-unstructured-data.s3-ap-northeast-1.amazonaws.com)... 52.219.4.91\n",
            "Connecting to basic-unstructured-data.s3-ap-northeast-1.amazonaws.com (basic-unstructured-data.s3-ap-northeast-1.amazonaws.com)|52.219.4.91|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4302872 (4.1M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   4.10M  4.75MB/s    in 0.9s    \n",
            "\n",
            "2021-08-08 11:13:47 (4.75 MB/s) - ‘data.zip’ saved [4302872/4302872]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/test.csv           \n",
            "  inflating: data/train.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ZB4T7WWXUSBK",
        "outputId": "6cf37799-241b-4dd5-e51e-470107d4df0a"
      },
      "source": [
        "train = pd.read_csv(\"data/train.csv\")\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>世界の映画祭で話題沸騰！インドネシアの壮絶バイオレンス・アクション『ザ・レイド』予告編が解禁...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>冬の女子会は特製「薬膳火鍋」に決定！美味しく食べてポッカポカ本格的な寒さを感じるこの季節。「...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012年・夏の浴衣スタイルのカギは帯！夏といえば浴衣。「そろそろ新しい浴衣を買おうかな」と...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>インタビュー：武田修宏さん「結婚するならセリエAクラスの女性がいい」少年時代から天才サッカー...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>「ポケモン」や「ONE PIECE」と同じ賞を獲得したキモかわいいキャラが全国に「クサマダラ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  世界の映画祭で話題沸騰！インドネシアの壮絶バイオレンス・アクション『ザ・レイド』予告編が解禁...      1\n",
              "1  冬の女子会は特製「薬膳火鍋」に決定！美味しく食べてポッカポカ本格的な寒さを感じるこの季節。「...      2\n",
              "2  2012年・夏の浴衣スタイルのカギは帯！夏といえば浴衣。「そろそろ新しい浴衣を買おうかな」と...      2\n",
              "3  インタビュー：武田修宏さん「結婚するならセリエAクラスの女性がいい」少年時代から天才サッカー...      2\n",
              "4  「ポケモン」や「ONE PIECE」と同じ賞を獲得したキモかわいいキャラが全国に「クサマダラ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Fs0ovMB5USBK",
        "outputId": "69b14259-9cfd-41b1-e503-dbf413610ddb"
      },
      "source": [
        "test = pd.read_csv(\"data/test.csv\")\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>【Sports Watch】元代表二人によるカメルーン戦の予想は？TBSの報道番組「NEWS...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>若槻千夏に激似!? 女子バレーのニューヒロイン新鍋理沙バレーボールのW杯が開幕し、ロンドン五...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ソーシャルの今が分かる！ヤフーの「話題なう」ってなに？ヤフーは、Twitterなどのソーシャ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>もう一つのユーロ2012美女ばかりの観客席には選手の彼女も!?連日連夜熱戦が展開されているサ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>元日本代表監督の岡田武史氏、杭州緑城の監督に決定日本代表の前監督である岡田武史氏が15日、中...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  【Sports Watch】元代表二人によるカメルーン戦の予想は？TBSの報道番組「NEWS...      3\n",
              "1  若槻千夏に激似!? 女子バレーのニューヒロイン新鍋理沙バレーボールのW杯が開幕し、ロンドン五...      3\n",
              "2  ソーシャルの今が分かる！ヤフーの「話題なう」ってなに？ヤフーは、Twitterなどのソーシャ...      0\n",
              "3  もう一つのユーロ2012美女ばかりの観客席には選手の彼女も!?連日連夜熱戦が展開されているサ...      3\n",
              "4  元日本代表監督の岡田武史氏、杭州緑城の監督に決定日本代表の前監督である岡田武史氏が15日、中...      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-t0okCllPIZ"
      },
      "source": [
        "# 正規化\n",
        "---\n",
        "\n",
        "```In [1]: import unicodedata\n",
        "In [2]: unicodedata.normalize(\"NFKC\", \"１②Ⅲ㈱㍗ﾊﾝｶｸ\")\n",
        "Out[2]: '12III(株)ワットハンカク'```\n",
        "\n",
        "と同様の処理をPandas Seriesの値に対して実施する場合は、「Series.str.normalize」を利用します。  \n",
        "「unicodedata.normalize」と同じ処理を実施してくれます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-0pRp-VUSBL"
      },
      "source": [
        "import unicodedata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "sAutw3gQUSBL",
        "outputId": "71cfc9a8-f0b6-43ba-ecc5-2c5b7d581b40"
      },
      "source": [
        "# ヒント\n",
        "text = \"私の誕生日は①月①①日のポッキーの日！!!?!\"\n",
        "unicodedata.normalize(\"NFKC\", text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'私の誕生日は1月11日のポッキーの日!!!?!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9evJ0JwVUSBM"
      },
      "source": [
        "### 問題1：train, testの列名textに対して、正規化してください。\n",
        "ヒント：\n",
        "df[列名].str.normalize(\"NFKC\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo4SR322USBM"
      },
      "source": [
        "# train[\"text\"] = train[\"text\"].str.normalize(\"NFKC\")\n",
        "# test[\"text\"] = test[\"text\"].str.normalize(\"NFKC\")\n",
        "\n",
        "def textnormalize(text):\n",
        "  return unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "train[\"text\"] = train[\"text\"].apply(textnormalize)\n",
        "test[\"text\"] = test[\"text\"].apply(textnormalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIM_squsa4xX"
      },
      "source": [
        "train[\"text\"] = train[\"text\"].str.normalize(\"NFKC\")\n",
        "test[\"text\"] = test[\"text\"].str.normalize(\"NFKC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "jA4JUE8dF-OX",
        "outputId": "3ad8dae6-0e63-40af-a014-63aa56206099"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>世界の映画祭で話題沸騰!インドネシアの壮絶バイオレンス・アクション『ザ・レイド』予告編が解禁...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>冬の女子会は特製「薬膳火鍋」に決定!美味しく食べてポッカポカ本格的な寒さを感じるこの季節。「...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012年・夏の浴衣スタイルのカギは帯!夏といえば浴衣。「そろそろ新しい浴衣を買おうかな」と...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>インタビュー:武田修宏さん「結婚するならセリエAクラスの女性がいい」少年時代から天才サッカー...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>「ポケモン」や「ONE PIECE」と同じ賞を獲得したキモかわいいキャラが全国に「クサマダラ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label\n",
              "0  世界の映画祭で話題沸騰!インドネシアの壮絶バイオレンス・アクション『ザ・レイド』予告編が解禁...      1\n",
              "1  冬の女子会は特製「薬膳火鍋」に決定!美味しく食べてポッカポカ本格的な寒さを感じるこの季節。「...      2\n",
              "2  2012年・夏の浴衣スタイルのカギは帯!夏といえば浴衣。「そろそろ新しい浴衣を買おうかな」と...      2\n",
              "3  インタビュー:武田修宏さん「結婚するならセリエAクラスの女性がいい」少年時代から天才サッカー...      2\n",
              "4  「ポケモン」や「ONE PIECE」と同じ賞を獲得したキモかわいいキャラが全国に「クサマダラ...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNekkrTuGKgU",
        "outputId": "d778a620-70e5-4d82-8c04-cd40fdece307"
      },
      "source": [
        "len(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "pZLNJ9b4GS2u",
        "outputId": "bd473afe-ff96-4168-d909-80bd85b3e4ff"
      },
      "source": [
        "train[\"text\"][992][:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'【終了しました】“女子力”のバイブル・映画『ガール』女性限定試写会に10組20名様をご招待映画『ガー'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu0eXa1UUSBM"
      },
      "source": [
        "## その他前処理\n",
        "---\n",
        "### 表記揺れの統一"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YgMZ_0bUSBM"
      },
      "source": [
        "### 問題2：train, testに\"【】\"と\"『』\"が混在しているので、\"「」\"で統一してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hIGJfR1USBN",
        "outputId": "40649120-8536-4876-dabb-2e7e2bfb6c60"
      },
      "source": [
        "# 以下のような文章をイメージしてください\n",
        "print(train[\"text\"][990][:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "『ミッション:8ミニッツ』繰り返される“8分間”の悪夢に隠された真実とは全米でこの春公開され、観る者\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8pC8cqNHUSBN",
        "outputId": "4edf15b1-9e07-41e4-b987-8f79b856478f"
      },
      "source": [
        "# ヒント\n",
        "unify_dic = {\n",
        "    '『': '「',\n",
        "    '』': '」',\n",
        "    '【': '「',\n",
        "    '】': '」'\n",
        "}\n",
        "text = train[\"text\"][992][:50]\n",
        "\n",
        "# 対応表を作ることができる\n",
        "dic_for_unify = str.maketrans(unify_dic)\n",
        "text.translate(dic_for_unify)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'「終了しました」“女子力”のバイブル・映画「ガール」女性限定試写会に10組20名様をご招待映画「ガー'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "LxiMUl5dUSBO",
        "outputId": "fe92cebf-355d-4000-f1bf-61f0f5e21e0b"
      },
      "source": [
        "# 関数化しておきましょう\n",
        "def unify_str(sentence):\n",
        "    dic_for_unify = str.maketrans(unify_dic)\n",
        "    sentence = sentence.translate(dic_for_unify)\n",
        "    return sentence\n",
        "\n",
        "unify_str(train[\"text\"][992][:50])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'「終了しました」“女子力”のバイブル・映画「ガール」女性限定試写会に10組20名様をご招待映画「ガー'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrYwWsY2USBO"
      },
      "source": [
        "# Enter your code here\n",
        "train[\"text\"] = train[\"text\"].apply(unify_str)\n",
        "test[\"text\"] = test[\"text\"].apply(unify_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj_ZCmozlPIi"
      },
      "source": [
        "# 形態素解析\n",
        "---\n",
        "\n",
        "MeCab を利用した形態素解析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-kfKBZTUSBO"
      },
      "source": [
        "### 問題3：train, testの各文書を形態素解析してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "keRNF7YXUSBP",
        "outputId": "3dadb092-7838-40fd-a312-fc0ff793861a"
      },
      "source": [
        "# ヒント\n",
        "mecab = MeCab.Tagger()\n",
        "\n",
        "# バグ回避用(おまじない)\n",
        "mecab.parse(\"\")\n",
        "\n",
        "result = []\n",
        "\n",
        "# node が None になるまで、node.next を辿り続ける\n",
        "node = mecab.parseToNode(\"本日も晴天なり\")\n",
        "while node:\n",
        "    result.append(node.surface)\n",
        "    result.append(\":\")\n",
        "    result.append(node.feature.split(\",\")[0])\n",
        "    result.append(\",\")\n",
        "    node = node.next\n",
        "\n",
        "\" \".join(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' : BOS/EOS , 本日 : 名詞 , も : 助詞 , 晴天 : 名詞 , なり : 助動詞 ,  : BOS/EOS ,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "8EoFFvR2USBP",
        "outputId": "0453e094-eda0-403a-e443-32c9dd5abdfb"
      },
      "source": [
        "# 関数化しておきましょう\n",
        "def get_morpheme(text):\n",
        "    result = []\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        result.append(node.surface)\n",
        "        node = node.next\n",
        "    return \" \".join(result)\n",
        "\n",
        "\n",
        "get_morpheme(\"本日も晴天なり。\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' 本日 も 晴天 なり 。 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV2QXhT7USBP"
      },
      "source": [
        "# Enter your code here\n",
        "# トークン化する\n",
        "\n",
        "mecab = MeCab.Tagger()\n",
        "# バグ回避用\n",
        "mecab.parse(\"\")\n",
        "\n",
        "train[\"text_tokenized\"] = train[\"text\"].apply(get_morpheme)\n",
        "test[\"text_tokenized\"] = test[\"text\"].apply(get_morpheme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzsNbN-LeT-F"
      },
      "source": [
        "**Janomeを利用した形態素解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqok1R8rhCeG",
        "outputId": "3aeb1d55-ef0e-4bfb-9ca9-88c727a644b8"
      },
      "source": [
        "!pip install janome"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 75.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw0-yrRphZ9B"
      },
      "source": [
        "from janome.tokenizer import Tokenizer"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXeqVk8eSHQ"
      },
      "source": [
        "t = Tokenizer(wakati=False)\n",
        "\n",
        "def get_morpheme_j(text):\n",
        "  result_j = []\n",
        "  for token in t.tokenize(text):\n",
        "    partOfSpeech = token.part_of_speech.split(\",\")[0]\n",
        "    if partOfSpeech not in [\"助詞\", \"助動詞\", \"記号\"]:\n",
        "      result_j.append(token.surface)\n",
        "  \" \".join(result_j)\n",
        "\n",
        "train[\"text_tokenized_j\"] = train[\"text\"].apply(get_morpheme)\n",
        "test[\"text_tokenized_j\"] = test[\"text\"].apply(get_morpheme)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "BpuWexHVhnOU",
        "outputId": "754ce669-5fb6-4f77-e04b-6910e838e934"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_tokenized</th>\n",
              "      <th>text_tokenized_j</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>世界の映画祭で話題沸騰!インドネシアの壮絶バイオレンス・アクション「ザ・レイド」予告編が解禁...</td>\n",
              "      <td>1</td>\n",
              "      <td>世界 の 映画 祭 で 話題 沸騰 ! インドネシア の 壮絶 バイオレンス・アクション ...</td>\n",
              "      <td>世界 の 映画 祭 で 話題 沸騰 ! インドネシア の 壮絶 バイオレンス・アクション ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>冬の女子会は特製「薬膳火鍋」に決定!美味しく食べてポッカポカ本格的な寒さを感じるこの季節。「...</td>\n",
              "      <td>2</td>\n",
              "      <td>冬 の 女子 会 は 特製 「 薬 膳 火 鍋 」 に 決定 ! 美味しく 食べ て ポッ...</td>\n",
              "      <td>冬 の 女子 会 は 特製 「 薬 膳 火 鍋 」 に 決定 ! 美味しく 食べ て ポッ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2012年・夏の浴衣スタイルのカギは帯!夏といえば浴衣。「そろそろ新しい浴衣を買おうかな」と...</td>\n",
              "      <td>2</td>\n",
              "      <td>2012 年 ・ 夏 の 浴衣 スタイル の カギ は 帯 ! 夏 と いえ ば 浴衣 。...</td>\n",
              "      <td>2012 年 ・ 夏 の 浴衣 スタイル の カギ は 帯 ! 夏 と いえ ば 浴衣 。...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>インタビュー:武田修宏さん「結婚するならセリエAクラスの女性がいい」少年時代から天才サッカー...</td>\n",
              "      <td>2</td>\n",
              "      <td>インタビュー : 武田 修 宏 さん 「 結婚 する なら セ リエ A クラス の 女性...</td>\n",
              "      <td>インタビュー : 武田 修 宏 さん 「 結婚 する なら セ リエ A クラス の 女性...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>「ポケモン」や「ONE PIECE」と同じ賞を獲得したキモかわいいキャラが全国に「クサマダラ...</td>\n",
              "      <td>1</td>\n",
              "      <td>「 ポケモン 」 や 「 ONE PIECE 」 と 同じ 賞 を 獲得 し た キモ か...</td>\n",
              "      <td>「 ポケモン 」 や 「 ONE PIECE 」 と 同じ 賞 を 獲得 し た キモ か...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                   text_tokenized_j\n",
              "0  世界の映画祭で話題沸騰!インドネシアの壮絶バイオレンス・アクション「ザ・レイド」予告編が解禁...  ...   世界 の 映画 祭 で 話題 沸騰 ! インドネシア の 壮絶 バイオレンス・アクション ...\n",
              "1  冬の女子会は特製「薬膳火鍋」に決定!美味しく食べてポッカポカ本格的な寒さを感じるこの季節。「...  ...   冬 の 女子 会 は 特製 「 薬 膳 火 鍋 」 に 決定 ! 美味しく 食べ て ポッ...\n",
              "2  2012年・夏の浴衣スタイルのカギは帯!夏といえば浴衣。「そろそろ新しい浴衣を買おうかな」と...  ...   2012 年 ・ 夏 の 浴衣 スタイル の カギ は 帯 ! 夏 と いえ ば 浴衣 。...\n",
              "3  インタビュー:武田修宏さん「結婚するならセリエAクラスの女性がいい」少年時代から天才サッカー...  ...   インタビュー : 武田 修 宏 さん 「 結婚 する なら セ リエ A クラス の 女性...\n",
              "4  「ポケモン」や「ONE PIECE」と同じ賞を獲得したキモかわいいキャラが全国に「クサマダラ...  ...   「 ポケモン 」 や 「 ONE PIECE 」 と 同じ 賞 を 獲得 し た キモ か...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Y4cR7ueSus"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CYuAmH0USBP"
      },
      "source": [
        "### 問題4：形態素解析した後に、['助詞', '助動詞', '記号'] のみ除外してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "yHuh28Z8USBQ",
        "outputId": "fa0e1987-1838-43be-e002-c11a4cc61b64"
      },
      "source": [
        "# ヒント\n",
        "mecab = MeCab.Tagger()\n",
        "\n",
        "# バグ回避用\n",
        "mecab.parse(\"\")\n",
        "\n",
        "result = []\n",
        "node = mecab.parseToNode(\"本日も晴天なり。\")\n",
        "while node:\n",
        "    if not node.feature.startswith(\"BOS/EOS\") \\\n",
        "   and not node.feature.startswith(\"助詞\") \\\n",
        "   and not node.feature.startswith(\"助動詞\") \\\n",
        "   and not node.feature.startswith(\"記号\"):\n",
        "        result.append(node.surface)\n",
        "    node = node.next\n",
        "\n",
        "print(result)\n",
        "\" \".join(result)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['本日', '晴天']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'本日 晴天'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5SpFEOQUSBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ac4d31-dceb-4d2d-b102-8d3011082c01"
      },
      "source": [
        "# 関数化しておきましょう\n",
        "def get_surfaces_list(text):\n",
        "    result = []\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        if node.feature.startswith(\"名詞\"):\n",
        "            result.append(node.surface)\n",
        "        node = node.next\n",
        "    return result\n",
        "\n",
        "train['text_tokenized_list'] = train['text'].apply(get_surfaces_list)\n",
        "all_token = pd.Series(train['text_tokenized_list'].sum())\n",
        "all_token.value_counts()[0:20]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     9697\n",
              "の     7762\n",
              "こと    7667\n",
              ")     6917\n",
              "日     6208\n",
              "!     5508\n",
              ":     5408\n",
              "人     4636\n",
              "1     4525\n",
              "月     4453\n",
              "-     4432\n",
              "2     4412\n",
              "3     4324\n",
              "映画    4272\n",
              "よう    3631\n",
              "年     3532\n",
              ".     3190\n",
              "ん     2707\n",
              "?     2700\n",
              "中     2600\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNNUHCiKUSBR"
      },
      "source": [
        "# Enter your code here\n",
        "mecab = MeCab.Tagger()\n",
        "# バグ回避用\n",
        "mecab.parse(\"\")\n",
        "\n",
        "train[\"text_tokenized\"] = train[\"text\"].apply(get_surfaces_list)\n",
        "test[\"text_tokenized\"] = test[\"text\"].apply(get_surfaces_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF8qofkvp8wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562388d6-16a0-4b4e-f592-eb596c57e6e8"
      },
      "source": [
        "train[\"text_tokenized\"].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [世界, 映画, 祭, 話題, 沸騰, !, インドネシア, 壮絶, バイオレンス・アクショ...\n",
              "1    [冬, 女子, 会, 特製, 薬, 膳, 火, 鍋, 決定, !, ポッカポカ, 本格, 的...\n",
              "2    [2012, 年, 夏, 浴衣, スタイル, カギ, 帯, !, 夏, 浴衣, 浴衣, 人,...\n",
              "3    [インタビュー, :, 武田, 修, 宏, さん, 結婚, セ, リエ, A, クラス, 女...\n",
              "4    [ポケモン, ONE, PIECE, 賞, 獲得, キモ, キャラ, 全国, クサマダラオオ...\n",
              "Name: text_tokenized, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRAm884glPIe"
      },
      "source": [
        "## その他前処理\n",
        "---\n",
        "### ありふれてて特徴が薄い単語を削除\n",
        "\n",
        "\"その\"や\"ため\"のような文字は、特徴が薄い単語だと考えることができるので除去する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hFVLc2jUSBR"
      },
      "source": [
        "### 問題5：\"その\"や\"ため\"を、train, testの各文書から除外してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U39Z4PnUSBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "432c2248-ac72-475f-bd71-71d0c3cb296e"
      },
      "source": [
        "# ヒント\n",
        "text = train[\"text\"][100][:50]\n",
        "print(text)\n",
        "\n",
        "stop_words = [\"その\", \"ため\"] # 適当な文字を設定\n",
        "for s in stop_words:\n",
        "    text = text.replace(s, \"\")\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "マンガのプロが選ぶ「女子の心にグッとくる10の名セリフ」 / 女子のためのマンガ特集いつでもガールズ\n",
            "マンガのプロが選ぶ「女子の心にグッとくる10の名セリフ」 / 女子ののマンガ特集いつでもガールズ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWnCPsr1eXzS"
      },
      "source": [
        "train[\"text_tokenized\"][150][:70]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_-SwPYqUSBS"
      },
      "source": [
        "# 関数化しておきましょう\n",
        "def remove_stop_words(sentence):\n",
        "    stop_words = [\"その\", \"ため\"] # 適当な文字を設定\n",
        "    for s in stop_words:\n",
        "        sentence = sentence.replace(s, '')\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBqPCbjdUSBS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "0bf0855d-f045-47fb-a524-7053175c10d8"
      },
      "source": [
        "# Enter your code here\n",
        "train[\"text_tokenized\"] = train[\"text_tokenized\"].apply(remove_stop_words)\n",
        "test[\"text_tokenized\"] = test[\"text_tokenized\"].apply(remove_stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-da8e53abfaaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Enter your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokenized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokenized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokenized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_tokenized\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_stop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4211\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4212\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4213\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-d100b96882db>\u001b[0m in \u001b[0;36mremove_stop_words\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"その\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ため\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 適当な文字を設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TuHL4-PUSBS"
      },
      "source": [
        "# 数値表現化\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP1mnGj_USBS"
      },
      "source": [
        "### 問題6：CountVectorizerでtrain, testを単語文書行列に変換してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2hjyQVmUSBS"
      },
      "source": [
        "※注意※  \n",
        "単語文章行列を学習データ, テストデータでそれぞれ生成します。  \n",
        "学習データに対してはfit_transform、そしてテストデータに対してはtransformを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qRjR7yDUSBT"
      },
      "source": [
        "# ヒント\n",
        "# token_pattern は、デフォルトだと1文字の単語を除外するので、除外しないように設定する\n",
        "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "# スペース区切りの文書をリストで与える\n",
        "X = vectorizer.fit_transform([\n",
        "    \"This is a pen\",\n",
        "    \"I am not a pen\",\n",
        "    \"Hello\"\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGiMR43lhlu5"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFnmSt3aUSBT"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQmx7drmUSBT"
      },
      "source": [
        "# これは、演習用に単語文書行列を DataFrame に変換して見やすくしてみるためのコードで、覚える必要はありません\n",
        "pd.DataFrame(X.toarray(), columns=[ x[0] for x in sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1]) ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjaFD3CxUSBT"
      },
      "source": [
        "# Enter your code here\n",
        "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "train_mat = vectorizer.fit_transform(train[\"text_tokenized\"])\n",
        "test_mat = vectorizer.transform(test[\"text_tokenized\"])\n",
        "\n",
        "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train[\"text_tokenized\"])\n",
        "X_test = vectorizer.transform(test[\"text_tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4MakLQnUSBT"
      },
      "source": [
        "### 問題7：TfidfVectorizerを使って、train, testを単語文書行列(TF-IDF適用済み)に変換してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_PD7-N5USBU"
      },
      "source": [
        "※注意※  \n",
        "単語文章行列を学習データ, テストデータでそれぞれ生成します。  \n",
        "学習データに対してはfit_transform、そしてテストデータに対してはtransformを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gkAyjiWUSBU"
      },
      "source": [
        "# ヒント\n",
        "# token_pattern は、デフォルトだと1文字の単語を除外するので、除外しないように設定する\n",
        "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "# スペース区切りの文書をリストで与える\n",
        "X = vectorizer.fit_transform([\n",
        "    \"This is a pen\",\n",
        "    \"I am not a pen\",\n",
        "    \"Hello\"\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VT-ue7tUSBU"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJgWcEMrUSBU"
      },
      "source": [
        "# これは、演習用に単語文書行列を DataFrame に変換して見やすくしてみるためのコードで、覚える必要はありません\n",
        "pd.DataFrame(X.toarray(), columns=[ x[0] for x in sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1]) ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbSsIgaIUSBU"
      },
      "source": [
        "# Enter your code here\n",
        "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "\n",
        "X_train_tridf = vectorizer.fit_transform(train[\"text_tokenized\"])\n",
        "X_test_tridf = vectorizer.transform(test[\"text_tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IuRFn5JlPKA"
      },
      "source": [
        "## 分析・機械学習\n",
        "---\n",
        "文書のベクトル化ができたら、後はこれまでやってきた機械学習と同様です。\n",
        "\n",
        "いくつかの分類器で、性能を比較してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYhz9L6DUSBV"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGgjN5cpUSBV"
      },
      "source": [
        "# Accuracy, Precision/Recall/F-score/Support, Confusion Matrix を表示\n",
        "def show_evaluation_metrics(y_true, y_pred):\n",
        "    print(\"Accuracy:\")\n",
        "    print(accuracy_score(y_true, y_pred))\n",
        "    print()\n",
        "    \n",
        "    print(\"Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    \n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5pl_QE9USBV"
      },
      "source": [
        "### 問題8：作成したランダムフォレストを用いて、TfidfVectorizerで数値化したtestの予測をしてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXgdQf8zUSBV"
      },
      "source": [
        "# ヒント：ロジスティック回帰モデルを作成する場合\n",
        "clf_lr = LogisticRegression(n_jobs=-1)\n",
        "clf_lr.fit(X_train_tridf, train[\"label\"])\n",
        "\n",
        "y_pred = clf_lr.predict(X_test_tridf)\n",
        "show_evaluation_metrics(y_pred, test[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zMMrgh41USBW"
      },
      "source": [
        "# Enter your code here\n",
        "clf_lr = RandomForestClassifier(n_jobs=-1)\n",
        "clf_lr.fit(X_train_tridf, train[\"label\"])\n",
        "\n",
        "y_pred = clf_lr.predict(X_test_tridf)\n",
        "show_evaluation_metrics(y_pred, test[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMFwAVxbUSBW"
      },
      "source": [
        "## 解答例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BRK6o3SUSBW"
      },
      "source": [
        "### 問題1：train, testの列名textに対して、正規化してください。\n",
        "ヒント：\n",
        "df[列名].str.normalize(\"NFKC\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hyJAisSUSBW"
      },
      "source": [
        "train[\"text\"] = train[\"text\"].str.normalize(\"NFKC\")\n",
        "test[\"text\"] = test[\"text\"].str.normalize(\"NFKC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1xwlFIvUSBW"
      },
      "source": [
        "### 問題2：train, testに\"【】\"と\"『』\"が混在しているので、\"「」\"で統一してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q6x6Y2QUSBW"
      },
      "source": [
        "def unify_str(sentence):\n",
        "    dic_for_unify = str.maketrans(unify_dic)\n",
        "    sentence = sentence.translate(dic_for_unify)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E9T2J8WUSBW"
      },
      "source": [
        "# Enter your code here\n",
        "train[\"text\"] = train[\"text\"].apply(unify_str)\n",
        "test[\"text\"] = test[\"text\"].apply(unify_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opcJOod0USBX"
      },
      "source": [
        "### 問題3：train, testの各文書を形態素解析してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2loGt6HkUSBX"
      },
      "source": [
        "def get_morpheme(text):\n",
        "    result = []\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        result.append(node.surface)\n",
        "        node = node.next\n",
        "    return \" \".join(result)\n",
        "\n",
        "get_morpheme(\"本日も晴天なり。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eFIq_fAUSBX"
      },
      "source": [
        "mecab = MeCab.Tagger()\n",
        "# バグ回避用\n",
        "mecab.parse(\"\")\n",
        "\n",
        "train['text_tokenized'] = train['text'].apply(get_morpheme)\n",
        "test['text_tokenized'] = test['text'].apply(get_morpheme)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rdmxeDcUSBX"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IJfJMiqUSBX"
      },
      "source": [
        "### 問題4：形態素解析した後に、['助詞', '助動詞', '記号'] のみ除外してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt_E5LNHUSBX"
      },
      "source": [
        "def get_surfaces(text):\n",
        "    result = []\n",
        "    node = mecab.parseToNode(text)\n",
        "    while node:\n",
        "        if not node.feature.startswith(\"BOS/EOS\") and not node.feature.startswith(\"助詞\") and not node.feature.startswith(\"助動詞\"):\n",
        "            result.append(node.surface)\n",
        "        node = node.next\n",
        "    return \" \".join(result)\n",
        "\n",
        "get_surfaces(\"本日も晴天なり。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WauAQeUyUSBX"
      },
      "source": [
        "mecab = MeCab.Tagger()\n",
        "# バグ回避用\n",
        "mecab.parse(\"\")\n",
        "\n",
        "train['text_tokenized'] = train['text'].apply(get_surfaces)\n",
        "test['text_tokenized'] = test['text'].apply(get_surfaces)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKTmNM2pUSBY"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj13u4GYUSBY"
      },
      "source": [
        "### 問題5：\"その\"や\"ため\"を、train, testの各文書から除外してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pkWfV2KUSBY"
      },
      "source": [
        "def remove_stop_words(sentence):\n",
        "    stop_words = [\"その\", \"ため\"] # 適当な文字を設定\n",
        "    for s in stop_words:\n",
        "        sentence = sentence.replace(s, '')\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsWRQp7FUSBZ"
      },
      "source": [
        "train[\"text_tokenized\"] = train[\"text_tokenized\"].apply(remove_stop_words)\n",
        "test[\"text_tokenized\"] = test[\"text_tokenized\"].apply(remove_stop_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKJNK8_YUSBZ"
      },
      "source": [
        "### 問題6：CountVectorizerでtrain, testを単語文書行列に変換してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4uk9ySrUSBZ"
      },
      "source": [
        "※注意※  \n",
        "単語文章行列を学習データ, テストデータでそれぞれ生成します。  \n",
        "学習データに対してはfit_transform、そしてテストデータに対してはtransformを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xsIee9USBZ"
      },
      "source": [
        "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "train_mat = vectorizer.fit_transform(train[\"text_tokenized\"])\n",
        "test_mat = vectorizer.transform(test[\"text_tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FffRj31dUSBZ"
      },
      "source": [
        "### 問題7：TfidfVectorizerを使って、train, testを単語文書行列(TF-IDF適用済み)に変換してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-SXdbf5USBZ"
      },
      "source": [
        "※注意※  \n",
        "単語文章行列を学習データ, テストデータでそれぞれ生成します。  \n",
        "学習データに対してはfit_transform、そしてテストデータに対してはtransformを使用します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fK57OecyUSBa"
      },
      "source": [
        "vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "train_mat = vectorizer.fit_transform(train[\"text_tokenized\"])\n",
        "test_mat = vectorizer.transform(test[\"text_tokenized\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ7HSLnTUSBa"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh82IpicUSBa"
      },
      "source": [
        "def show_evaluation_metrics(y_true, y_pred):\n",
        "    print(\"Accuracy:\")\n",
        "    print(accuracy_score(y_true, y_pred))\n",
        "    print()\n",
        "    \n",
        "    print(\"Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    \n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaNBgLHVUSBa"
      },
      "source": [
        "### 問題8：作成したランダムフォレストを用いて、TfidfVectorizerで数値化したtestの予測をしてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "z_FBS7b8USBa"
      },
      "source": [
        "clf_rf = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
        "clf_rf.fit(train_mat, train[\"label\"])\n",
        "\n",
        "y_pred = clf_rf.predict(test_mat)\n",
        "show_evaluation_metrics(y_pred, test[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuRFDmWJUSBa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}